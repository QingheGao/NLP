{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19465\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "\n",
    "# Initialize variables: The original dataset contains one token per line.\n",
    "# For tagging, we need to process it sentence by sentence\n",
    "original_dataset = []\n",
    "dataset_for_tagging =[]\n",
    "\n",
    "# Path to the CoNLL file and to the new output\n",
    "#### Make sure to do this with both: the training and the test file!!! ###\n",
    "\n",
    "# path_conll = \"/Users/hernando/Desktop/NLP/assignment3/error_propagation 2/data/conll_test_cap.txt\"\n",
    "\n",
    "# path_conll_nltk_tags ='/Users/hernando/Desktop/NLP/assignment3/error_propagation 2/data/conll_test_cap_nltktags.txt'\n",
    "\n",
    "path_conll = \"/Users/hernando/Desktop/NLP/assignment3/error_propagation 2/data/conll_train_cap.txt\"\n",
    "\n",
    "path_conll_nltk_tags ='/Users/hernando/Desktop/NLP/assignment3/error_propagation 2/data/conll_train_cap_nltktags.txt'\n",
    "\n",
    "with (open(path_conll)) as f:\n",
    "    csv_file = csv.reader(f, delimiter='\\t', quotechar=None)\n",
    "\n",
    "    # For each sentence, we want to store the following information:\n",
    "    # words, pos-tags (from the MBT Tagger), ne-labels\n",
    "    words, mbt_tags, labels = [], [], []\n",
    "\n",
    "    for idx, row in enumerate(csv_file):\n",
    "        # Keep all lines unchanged here\n",
    "        original_dataset.append(row)\n",
    "\n",
    "        # Empty rows indicate sentence boundaries. We add the collected information to the dataset\n",
    "        if len(row) == 0:\n",
    "\n",
    "            # Make sure that everything has the same length\n",
    "            assert len(words) == len(mbt_tags) == len(labels)\n",
    "\n",
    "            # Add the information for the current sentence to the dataset\n",
    "            # Each sentence is a tuple of words, tags, and labels\n",
    "            dataset_for_tagging.append((words, mbt_tags, labels))\n",
    "\n",
    "            # Re-initialize the variables\n",
    "            words, mbt_tags, labels = [], [], []\n",
    "\n",
    "        else:\n",
    "            word, previous, casing, pos, chunk, label = row\n",
    "            word = str(row[0])\n",
    "            mbt_tag = str(row[3])\n",
    "            label = str(row[5])\n",
    "\n",
    "            words.append(word)\n",
    "            mbt_tags.append(pos)\n",
    "            labels.append(label)\n",
    "\n",
    "# The last empty line is not processed by the reader, so we need to add the last sentence here.\n",
    "if not len(words) == 0:\n",
    "    dataset_for_tagging.append((words, mbt_tags, labels))\n",
    "\n",
    "##### Your work starts HERE ####\n",
    "\n",
    "# Initialize some counting variables\n",
    "\n",
    "differnet_pos = 0\n",
    "\n",
    "differentlist=[]\n",
    "wordslist = []\n",
    "all_nltk_tags =[]\n",
    "\n",
    "for sentence in dataset_for_tagging:\n",
    "    # Split the tuple into the different categories\n",
    "    words, mbt_tags, labels = sentence\n",
    "\n",
    "    # Get the new POS-tags\n",
    "    # The POS-tagger outputs a list of tuples [(word, POS-tag),...]\n",
    "    nltk_output = nltk.pos_tag(words)\n",
    "    # We only want to extract the tags\n",
    "    nltk_tags = [tag for word, tag in nltk_output]\n",
    "    all_nltk_tags.append(nltk_tags)\n",
    "\n",
    "    # Now do your analysis on the words and the tags\n",
    "    # Make sure to ignore the word \"-DOCSTART-\" because it just indicates document boundaries\n",
    "    for i in range(len(mbt_tags)):\n",
    "        if mbt_tags[i] == '-X-':\n",
    "            continue\n",
    "        else:\n",
    "            if mbt_tags[i] != nltk_tags[i]:\n",
    "                differentlist.append((mbt_tags[i],nltk_tags[i]))\n",
    "                wordslist.append(words[i])\n",
    "                differnet_pos +=1\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Output your results here: ####\n",
    "\n",
    "print(differnet_pos)\n",
    "\n",
    "# Now, let's output the CoNLL data with the output for the POS column replaced with the nltk tags\n",
    "with (open(path_conll_nltk_tags, \"w\")) as outfile:\n",
    "    # Initialize ids\n",
    "    sentence_id = 0\n",
    "    word_id = 0\n",
    "\n",
    "    for row in original_dataset:\n",
    "        # Add a new line after each sentence\n",
    "        if len(row) == 0:\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "            # update variables\n",
    "            sentence_id += 1\n",
    "            word_id = 0\n",
    "\n",
    "        else:\n",
    "            # Do not change the tag for document boundaries\n",
    "            if word == \"-DOCSTART-\":\n",
    "                outfile.write(\"\\t\".join(row))\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "            # Replace the tag for all other tokens\n",
    "            else:\n",
    "                new_row = [x for x in row]\n",
    "\n",
    "                nltk_tag = all_nltk_tags[sentence_id][word_id]\n",
    "                new_row[3] = nltk_tag\n",
    "\n",
    "                outfile.write(\"\\t\".join(new_row))\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "            word_id += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('NNP', 'JJ'): 731,\n",
       "         ('NN', 'NNS'): 240,\n",
       "         ('NN', 'JJ'): 748,\n",
       "         ('NN', 'VB'): 149,\n",
       "         ('\"', 'NN'): 869,\n",
       "         ('\"', 'VBZ'): 55,\n",
       "         ('NNP', 'NN'): 661,\n",
       "         ('FW', 'NN'): 54,\n",
       "         ('JJ', 'RBR'): 3,\n",
       "         ('JJR', 'NNP'): 40,\n",
       "         ('VBN', 'VBD'): 420,\n",
       "         ('VBZ', 'NNS'): 300,\n",
       "         ('WDT', 'IN'): 18,\n",
       "         ('VB', 'NN'): 325,\n",
       "         ('VBD', 'VBN'): 380,\n",
       "         ('RB', 'EX'): 11,\n",
       "         ('NNP', 'FW'): 24,\n",
       "         ('JJ', 'NNP'): 962,\n",
       "         ('\"', 'NNP'): 519,\n",
       "         ('JJ', 'NN'): 673,\n",
       "         ('NNP', 'RB'): 54,\n",
       "         ('NNS', 'VB'): 6,\n",
       "         ('\"', 'VB'): 232,\n",
       "         ('JJ', 'VB'): 35,\n",
       "         ('CD', 'JJ'): 1836,\n",
       "         ('VBZ', 'NNP'): 60,\n",
       "         ('DT', 'VBD'): 3,\n",
       "         ('VBD', 'NN'): 65,\n",
       "         ('NN', 'RB'): 63,\n",
       "         ('VBG', 'NNP'): 98,\n",
       "         ('NNP', 'VB'): 16,\n",
       "         ('JJ', 'RB'): 115,\n",
       "         ('RB', 'JJ'): 132,\n",
       "         ('NNP', 'NNS'): 156,\n",
       "         ('JJ', 'VBG'): 90,\n",
       "         ('WDT', 'DT'): 4,\n",
       "         ('NNPS', 'NNP'): 406,\n",
       "         ('NNP', 'NNPS'): 133,\n",
       "         ('RB', 'NN'): 102,\n",
       "         ('IN', 'NNS'): 8,\n",
       "         ('RB', 'IN'): 148,\n",
       "         ('JJ', 'NNS'): 28,\n",
       "         ('SYM', 'NNP'): 136,\n",
       "         ('NNP', 'CD'): 382,\n",
       "         ('DT', 'IN'): 23,\n",
       "         ('RBR', 'JJR'): 46,\n",
       "         ('NN', 'NNP'): 1253,\n",
       "         ('NNS', 'NNP'): 599,\n",
       "         ('VBP', 'NNP'): 16,\n",
       "         ('TO', 'NNP'): 39,\n",
       "         ('IN', 'NNP'): 179,\n",
       "         ('VB', 'JJ'): 60,\n",
       "         ('VBG', 'JJ'): 37,\n",
       "         ('CD', 'NNP'): 82,\n",
       "         ('SYM', 'NN'): 157,\n",
       "         ('VBN', 'NNP'): 68,\n",
       "         ('DT', 'NNP'): 42,\n",
       "         ('MD', 'NNP'): 4,\n",
       "         ('VBD', 'NNP'): 41,\n",
       "         ('VB', 'NNP'): 204,\n",
       "         ('VBG', 'NN'): 137,\n",
       "         ('CD', 'VBD'): 47,\n",
       "         ('NNS', 'VBZ'): 88,\n",
       "         ('RP', 'RB'): 73,\n",
       "         ('VB', 'VBN'): 11,\n",
       "         ('\"', 'IN'): 145,\n",
       "         ('NN', 'VBP'): 75,\n",
       "         ('VB', 'VBP'): 150,\n",
       "         ('NNS', 'CD'): 14,\n",
       "         ('NNS', 'NN'): 212,\n",
       "         ('VBD', 'VB'): 12,\n",
       "         ('NN', 'MD'): 11,\n",
       "         ('\"', 'VBP'): 60,\n",
       "         ('VBN', 'JJ'): 173,\n",
       "         ('\"', 'JJ'): 173,\n",
       "         ('RB', 'RP'): 74,\n",
       "         ('SYM', '$'): 3,\n",
       "         ('RP', 'IN'): 74,\n",
       "         ('\"', 'CC'): 20,\n",
       "         ('DT', 'WDT'): 3,\n",
       "         ('JJ', 'VBD'): 63,\n",
       "         ('NNP', '$'): 100,\n",
       "         ('IN', 'WDT'): 22,\n",
       "         ('UH', 'NNP'): 23,\n",
       "         ('JJR', 'JJ'): 6,\n",
       "         ('NN', 'VBG'): 185,\n",
       "         ('IN', 'RP'): 71,\n",
       "         ('VBP', 'NN'): 93,\n",
       "         ('NN', 'VBD'): 59,\n",
       "         ('NNPS', 'NNS'): 72,\n",
       "         ('NNP', 'VBP'): 16,\n",
       "         ('NNP', 'VBG'): 42,\n",
       "         ('JJ', 'CD'): 230,\n",
       "         ('VBN', 'NN'): 86,\n",
       "         (\"''\", 'POS'): 8,\n",
       "         ('CD', 'NN'): 150,\n",
       "         ('RBR', 'RB'): 10,\n",
       "         ('\"', 'RB'): 19,\n",
       "         ('VBP', 'JJ'): 17,\n",
       "         ('JJ', 'VBN'): 158,\n",
       "         ('VB', 'VBD'): 88,\n",
       "         ('IN', 'RB'): 76,\n",
       "         ('RB', 'RBR'): 23,\n",
       "         ('FW', 'NNP'): 61,\n",
       "         ('RB', 'NNS'): 9,\n",
       "         ('VBZ', 'NN'): 11,\n",
       "         ('VBP', 'VB'): 42,\n",
       "         ('WDT', 'NNP'): 10,\n",
       "         ('NNP', 'VBN'): 25,\n",
       "         ('RB', 'NNP'): 103,\n",
       "         ('LS', 'CD'): 13,\n",
       "         ('SYM', 'VBP'): 19,\n",
       "         ('RP', 'NNP'): 19,\n",
       "         ('TO', 'CD'): 24,\n",
       "         ('CD', 'LS'): 17,\n",
       "         ('VBD', 'VBP'): 30,\n",
       "         ('NN', 'CD'): 33,\n",
       "         ('VBP', 'VBZ'): 9,\n",
       "         ('NNP', 'VBZ'): 34,\n",
       "         ('NNP', 'PRP'): 1,\n",
       "         ('NNPS', 'SYM'): 1,\n",
       "         ('NN', 'VBZ'): 23,\n",
       "         ('PRP$', 'NNP'): 6,\n",
       "         ('JJS', 'NN'): 7,\n",
       "         ('CD', 'NNS'): 146,\n",
       "         ('IN', 'FW'): 56,\n",
       "         ('NN', 'JJR'): 10,\n",
       "         ('JJR', 'NN'): 22,\n",
       "         ('NN', 'FW'): 7,\n",
       "         ('CD', 'SYM'): 53,\n",
       "         ('NNP', 'CC'): 14,\n",
       "         ('TO', 'NN'): 5,\n",
       "         ('PRP', 'NNP'): 18,\n",
       "         ('NN', 'DT'): 8,\n",
       "         ('IN', 'NN'): 15,\n",
       "         ('NNS', 'JJ'): 34,\n",
       "         ('VBD', 'JJ'): 43,\n",
       "         ('JJS', 'JJ'): 2,\n",
       "         ('DT', 'NN'): 6,\n",
       "         ('IN', 'VBP'): 5,\n",
       "         ('JJ', 'DT'): 3,\n",
       "         ('NNPS', 'VBZ'): 2,\n",
       "         ('NNS', 'VBD'): 7,\n",
       "         ('VBZ', 'POS'): 11,\n",
       "         ('\"', 'FW'): 21,\n",
       "         ('JJS', 'NNP'): 17,\n",
       "         ('JJR', 'RBR'): 37,\n",
       "         ('FW', 'NNS'): 3,\n",
       "         ('RBS', 'JJS'): 4,\n",
       "         ('CD', 'VBP'): 3,\n",
       "         ('NNS', 'SYM'): 2,\n",
       "         ('RB', 'VBP'): 3,\n",
       "         ('NNS', 'NNPS'): 23,\n",
       "         ('CC', 'RB'): 7,\n",
       "         ('JJ', 'CC'): 2,\n",
       "         ('VBN', 'VB'): 5,\n",
       "         ('VB', 'IN'): 3,\n",
       "         ('RB', 'VBZ'): 3,\n",
       "         ('SYM', 'VBZ'): 22,\n",
       "         ('VBD', 'VBZ'): 2,\n",
       "         ('SYM', 'JJ'): 62,\n",
       "         ('NNP', 'JJS'): 7,\n",
       "         ('\"', 'VBD'): 17,\n",
       "         ('NN', 'JJS'): 16,\n",
       "         ('NN', 'VBN'): 28,\n",
       "         ('VBP', 'IN'): 2,\n",
       "         ('NN', 'IN'): 16,\n",
       "         ('\"', 'EX'): 4,\n",
       "         ('MD', 'JJ'): 1,\n",
       "         ('WDT', 'NN'): 2,\n",
       "         ('IN', 'DT'): 18,\n",
       "         ('PRP$', 'PRP'): 11,\n",
       "         ('NNS', 'RB'): 6,\n",
       "         ('PRP', 'PRP$'): 20,\n",
       "         ('\"', 'NNS'): 23,\n",
       "         ('\"', 'VBN'): 7,\n",
       "         ('VBP', 'VBD'): 8,\n",
       "         ('DT', 'CC'): 3,\n",
       "         ('JJ', 'VBP'): 18,\n",
       "         ('VBN', 'RB'): 3,\n",
       "         ('VB', 'RB'): 6,\n",
       "         ('MD', 'VBP'): 6,\n",
       "         ('VBD', 'RB'): 4,\n",
       "         ('VBZ', 'VB'): 3,\n",
       "         ('POS', \"''\"): 2,\n",
       "         ('VBZ', 'RB'): 1,\n",
       "         ('RB', 'VB'): 8,\n",
       "         ('JJS', 'RBS'): 3,\n",
       "         ('RB', 'VBN'): 5,\n",
       "         ('SYM', 'VB'): 3,\n",
       "         ('SYM', 'IN'): 2,\n",
       "         ('TO', '$'): 3,\n",
       "         ('SYM', 'VBD'): 20,\n",
       "         ('IN', 'JJ'): 20,\n",
       "         ('JJ', 'IN'): 16,\n",
       "         ('VB', 'RP'): 1,\n",
       "         ('DT', 'JJ'): 4,\n",
       "         ('CD', 'VBZ'): 14,\n",
       "         ('CC', 'NNP'): 17,\n",
       "         ('JJ', 'MD'): 1,\n",
       "         ('NNP', 'VBD'): 29,\n",
       "         ('(', 'RB'): 4,\n",
       "         (')', 'NN'): 4,\n",
       "         ('VBZ', 'NNPS'): 1,\n",
       "         ('NNS', 'VBN'): 2,\n",
       "         ('NNP', 'DT'): 13,\n",
       "         ('NNS', 'JJR'): 1,\n",
       "         ('RB', 'CD'): 6,\n",
       "         ('NNPS', 'JJ'): 1,\n",
       "         ('JJ', 'RP'): 2,\n",
       "         ('JJ', 'JJR'): 7,\n",
       "         ('UH', 'NN'): 2,\n",
       "         ('CC', 'NN'): 1,\n",
       "         ('RP', 'JJ'): 1,\n",
       "         ('\"', 'CD'): 3,\n",
       "         ('RB', 'DT'): 6,\n",
       "         ('\"', 'VBG'): 1,\n",
       "         ('VBN', 'CD'): 1,\n",
       "         ('PDT', 'RB'): 2,\n",
       "         ('VB', 'VBZ'): 5,\n",
       "         ('JJ', 'PDT'): 2,\n",
       "         ('POS', 'VBZ'): 2,\n",
       "         ('JJ', 'FW'): 2,\n",
       "         ('DT', 'PDT'): 7,\n",
       "         ('WP', 'WDT'): 4,\n",
       "         ('RB', 'MD'): 1,\n",
       "         ('IN', 'VBZ'): 1,\n",
       "         ('WDT', 'WP'): 5,\n",
       "         ('$', 'VB'): 1,\n",
       "         ('EX', 'RB'): 4,\n",
       "         ('WP', 'IN'): 1,\n",
       "         ('$', 'NNP'): 10,\n",
       "         ('VBG', 'SYM'): 10,\n",
       "         ('IN', 'CC'): 5,\n",
       "         ('CD', 'RB'): 6,\n",
       "         ('NN', '$'): 5,\n",
       "         ('VBN', 'VBP'): 4,\n",
       "         ('VBZ', 'JJ'): 8,\n",
       "         ('FW', 'RB'): 1,\n",
       "         ('NNP', 'IN'): 18,\n",
       "         ('PRP', 'CD'): 3,\n",
       "         ('VB', '$'): 4,\n",
       "         ('NNP', 'JJR'): 9,\n",
       "         ('RB', 'VBD'): 5,\n",
       "         ('$', 'JJ'): 2,\n",
       "         ('FW', 'JJ'): 10,\n",
       "         ('WRB', 'NNP'): 1,\n",
       "         ('WRB', 'NN'): 1,\n",
       "         ('PDT', 'JJ'): 1,\n",
       "         ('VB', 'VBG'): 2,\n",
       "         ('\"', 'JJS'): 1,\n",
       "         ('VB', 'JJS'): 1,\n",
       "         ('IN', 'VBN'): 2,\n",
       "         ('RB', 'CC'): 5,\n",
       "         ('JJ', 'VBZ'): 2,\n",
       "         ('NNS', 'FW'): 2,\n",
       "         ('VBN', '$'): 2,\n",
       "         ('NN', 'PDT'): 6,\n",
       "         ('VBG', '$'): 2,\n",
       "         ('DT', 'RB'): 5,\n",
       "         ('NNPS', 'VBN'): 1,\n",
       "         ('RB', 'JJR'): 6,\n",
       "         ('RB', 'JJS'): 5,\n",
       "         ('NN', 'NNPS'): 1,\n",
       "         ('MD', 'VB'): 1,\n",
       "         ('\"', 'PDT'): 3,\n",
       "         ('\"', 'MD'): 1,\n",
       "         ('RB', 'PDT'): 2,\n",
       "         ('NN', 'SYM'): 1,\n",
       "         ('FW', 'VBD'): 1,\n",
       "         ('UH', 'NNS'): 1,\n",
       "         ('FW', 'VBP'): 3,\n",
       "         ('VBD', 'WDT'): 1,\n",
       "         ('FW', 'IN'): 5,\n",
       "         ('PRP', 'VBN'): 1,\n",
       "         ('NN', 'RP'): 5,\n",
       "         ('FW', 'VBZ'): 2,\n",
       "         ('NNS', 'VBP'): 5,\n",
       "         ('CC', 'IN'): 3,\n",
       "         ('CC', 'JJR'): 2,\n",
       "         ('FW', '$'): 2,\n",
       "         ('DT', 'PRP'): 2,\n",
       "         ('CD', '$'): 3,\n",
       "         ('VBG', 'VBP'): 2,\n",
       "         ('PRP$', 'JJ'): 1,\n",
       "         ('SYM', 'CD'): 6,\n",
       "         ('\"', '$'): 2,\n",
       "         ('\"', \"''\"): 3,\n",
       "         ('PRP', 'NNS'): 1,\n",
       "         ('NNPS', 'NN'): 2,\n",
       "         ('IN', '$'): 1,\n",
       "         ('PRP', 'JJ'): 4,\n",
       "         ('(', 'NNP'): 1,\n",
       "         (')', 'NNP'): 1,\n",
       "         ('NNP', 'WP'): 1,\n",
       "         ('PDT', 'DT'): 2,\n",
       "         ('VBD', 'MD'): 1,\n",
       "         ('NNP', 'POS'): 3,\n",
       "         ('CC', 'DT'): 2,\n",
       "         ('VBD', 'NNS'): 3,\n",
       "         ('MD', 'VBD'): 1,\n",
       "         ('NNS', 'CC'): 1,\n",
       "         ('PRP', 'NN'): 5,\n",
       "         ('PRP', 'POS'): 1,\n",
       "         ('TO', 'VBP'): 1,\n",
       "         ('JJ', '$'): 1,\n",
       "         ('VBP', 'VBN'): 1,\n",
       "         ('VB', 'NNS'): 1,\n",
       "         ('JJS', 'VB'): 1,\n",
       "         ('NNP', 'WDT'): 2,\n",
       "         ('WP', 'NN'): 1,\n",
       "         ('NN', 'POS'): 3,\n",
       "         ('SYM', 'FW'): 1,\n",
       "         ('VBD', 'CC'): 4,\n",
       "         ('VBP', 'PRP'): 1,\n",
       "         ('IN', 'VBD'): 1,\n",
       "         ('PRP', 'VB'): 1,\n",
       "         ('VBP', 'JJS'): 1,\n",
       "         ('VBP', 'RB'): 1,\n",
       "         ('VBN', 'NNS'): 4,\n",
       "         ('VBG', 'NNS'): 1,\n",
       "         ('JJR', 'VB'): 3,\n",
       "         ('UH', 'DT'): 1,\n",
       "         ('CD', 'VB'): 1,\n",
       "         ('JJR', 'VBN'): 1,\n",
       "         ('VB', 'JJR'): 2,\n",
       "         ('JJR', 'RB'): 1,\n",
       "         ('DT', 'UH'): 1,\n",
       "         ('RB', 'PRP$'): 1,\n",
       "         ('RB', 'UH'): 1,\n",
       "         (',', 'NN'): 1,\n",
       "         ('VBZ', 'VBP'): 1,\n",
       "         ('|NN||SYM|', 'VBZ'): 2,\n",
       "         ('|NN||SYM|', 'NNP'): 2,\n",
       "         ('SYM', 'RB'): 7,\n",
       "         ('UH', 'VB'): 1,\n",
       "         ('WP', 'DT'): 1,\n",
       "         ('RBS', 'NNP'): 1,\n",
       "         ('JJ', 'JJS'): 2,\n",
       "         ('MD', 'NN'): 2,\n",
       "         ('RBR', 'JJ'): 1,\n",
       "         ('NN', 'CC'): 1,\n",
       "         ('NNS', 'IN'): 1,\n",
       "         ('VBD', 'SYM'): 1,\n",
       "         ('IN', 'CD'): 1,\n",
       "         ('FW', 'VB'): 1,\n",
       "         ('PDT', 'CC'): 1})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.Counter(differentlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'European': 27,\n",
       "         'lamb': 1,\n",
       "         'cow': 5,\n",
       "         'sheep': 6,\n",
       "         '\"': 2178,\n",
       "         'van': 17,\n",
       "         'der': 9,\n",
       "         'further': 12,\n",
       "         'Fischler': 1,\n",
       "         'proposed': 2,\n",
       "         'measures': 1,\n",
       "         'that': 85,\n",
       "         'contract': 4,\n",
       "         'animal': 5,\n",
       "         'questioned': 2,\n",
       "         'there': 14,\n",
       "         'Spanish': 8,\n",
       "         'de': 33,\n",
       "         'EU': 3,\n",
       "         'veterinary': 1,\n",
       "         'Sheep': 2,\n",
       "         'cattle': 3,\n",
       "         'denied': 1,\n",
       "         'collapsed': 2,\n",
       "         'mad': 2,\n",
       "         '1996-08-22': 103,\n",
       "         'handwritten': 1,\n",
       "         'Ai': 4,\n",
       "         'no': 8,\n",
       "         'penned': 1,\n",
       "         'English': 24,\n",
       "         'inlaid': 1,\n",
       "         'overdose': 1,\n",
       "         'aged': 5,\n",
       "         'spoils': 1,\n",
       "         'atmosphere': 1,\n",
       "         'BEIJING': 9,\n",
       "         'Ukraine': 3,\n",
       "         'Taiwanese': 2,\n",
       "         'only': 11,\n",
       "         'right': 11,\n",
       "         'Reuters': 39,\n",
       "         'visiting': 2,\n",
       "         'People': 4,\n",
       "         'Daily': 2,\n",
       "         'quoted': 9,\n",
       "         'renegade': 2,\n",
       "         'towards': 9,\n",
       "         'official': 22,\n",
       "         'executive': 9,\n",
       "         'on': 29,\n",
       "         'German': 21,\n",
       "         'pct': 30,\n",
       "         'yr': 6,\n",
       "         '/': 235,\n",
       "         'FRANKFURT': 4,\n",
       "         'more': 38,\n",
       "         'Third': 3,\n",
       "         'Seat': 1,\n",
       "         'SOCIALISTS': 1,\n",
       "         'GIVE': 1,\n",
       "         'TO': 27,\n",
       "         'FOR': 23,\n",
       "         'ELECTIONS': 1,\n",
       "         'ATHENS': 10,\n",
       "         'socialist': 2,\n",
       "         'Costas': 7,\n",
       "         'Simitis': 3,\n",
       "         'snap': 6,\n",
       "         '3311812-4': 7,\n",
       "         'BayerVB': 1,\n",
       "         'following': 8,\n",
       "         'lead': 17,\n",
       "         'COUPON': 2,\n",
       "         'PAY': 6,\n",
       "         'FEES': 2,\n",
       "         '+20': 1,\n",
       "         'FREQ': 1,\n",
       "         '=': 11,\n",
       "         '1-10-100': 2,\n",
       "         'LIMITS': 2,\n",
       "         'CA': 1,\n",
       "         'NO': 4,\n",
       "         'FORCE': 3,\n",
       "         'MAJ': 2,\n",
       "         'LAW': 1,\n",
       "         'TAX': 3,\n",
       "         'PROVS': 2,\n",
       "         'STANDARD': 2,\n",
       "         'SELL': 2,\n",
       "         'UNDERLYING': 1,\n",
       "         'GOVT': 1,\n",
       "         'BOND': 3,\n",
       "         'BAYERISCHE': 1,\n",
       "         'IS': 2,\n",
       "         'JOINT': 1,\n",
       "         'MANAGER': 4,\n",
       "         '+44': 14,\n",
       "         'Venantius': 1,\n",
       "         'sets': 5,\n",
       "         'floating-rate': 1,\n",
       "         'AGENCY': 1,\n",
       "         '3M': 1,\n",
       "         'LAST': 4,\n",
       "         'REOFFER': 1,\n",
       "         'LISTING': 3,\n",
       "         'FR': 1,\n",
       "         'ISSUED': 1,\n",
       "         'OFF': 1,\n",
       "         'Port': 3,\n",
       "         'waiting': 2,\n",
       "         'down': 48,\n",
       "         'JERUSALEM': 13,\n",
       "         'outgoing': 2,\n",
       "         'conducted': 1,\n",
       "         'Rabinovich': 1,\n",
       "         'Israeli': 31,\n",
       "         'Syria': 1,\n",
       "         'test': 3,\n",
       "         'fired': 2,\n",
       "         'al-': 1,\n",
       "         'Assad': 1,\n",
       "         'Heights': 1,\n",
       "         'Golan': 1,\n",
       "         'spread': 1,\n",
       "         'expect': 3,\n",
       "         'peace': 1,\n",
       "         'forbid': 1,\n",
       "         'calming': 1,\n",
       "         'signal': 2,\n",
       "         'Netanyahu': 6,\n",
       "         'TUNIS': 3,\n",
       "         'visas': 3,\n",
       "         'stranded': 3,\n",
       "         'kept': 4,\n",
       "         'will': 6,\n",
       "         'labour': 3,\n",
       "         'estimated': 14,\n",
       "         'deep': 1,\n",
       "         'Kurds': 7,\n",
       "         'Resistance': 1,\n",
       "         'Iranian': 3,\n",
       "         'oppositions': 1,\n",
       "         'Iraqi': 54,\n",
       "         'Kurdish': 20,\n",
       "         'had': 7,\n",
       "         'out': 91,\n",
       "         'U.S.-sponsored': 1,\n",
       "         'mount': 1,\n",
       "         'Saudi': 10,\n",
       "         'riyal': 2,\n",
       "         'steady': 3,\n",
       "         '5-1/2': 1,\n",
       "         '5-5/8': 1,\n",
       "         '5-3/4': 1,\n",
       "         'Palestinian': 24,\n",
       "         'over': 26,\n",
       "         'PLO': 1,\n",
       "         'Arafat': 1,\n",
       "         'Peres': 7,\n",
       "         'home': 6,\n",
       "         'Afghan': 11,\n",
       "         'Taleban': 2,\n",
       "         'travel': 1,\n",
       "         'aircrew': 1,\n",
       "         'hand': 3,\n",
       "         'since': 3,\n",
       "         'silent': 1,\n",
       "         'That': 8,\n",
       "         'Aerostan': 1,\n",
       "         'Russian': 23,\n",
       "         'land': 2,\n",
       "         'Rabbani': 1,\n",
       "         'free': 4,\n",
       "         'failed': 6,\n",
       "         'armed': 14,\n",
       "         'Ilyushin': 1,\n",
       "         '76': 1,\n",
       "         'ultra-nationalist': 1,\n",
       "         'Liberal': 2,\n",
       "         'Democratic': 21,\n",
       "         'end': 3,\n",
       "         'firm': 3,\n",
       "         'held': 14,\n",
       "         'PRESS': 24,\n",
       "         'Aug': 19,\n",
       "         'vouch': 16,\n",
       "         'THAWRA': 1,\n",
       "         'Iraq': 2,\n",
       "         'liberal': 1,\n",
       "         'fair': 3,\n",
       "         'arrives': 4,\n",
       "         'Umm': 1,\n",
       "         'BEIRUT': 3,\n",
       "         'AN-NAHAR': 2,\n",
       "         'serve': 3,\n",
       "         'AS-SAFIR': 2,\n",
       "         'Parliament': 3,\n",
       "         '..': 10,\n",
       "         'AL-ANWAR': 2,\n",
       "         'Continued': 1,\n",
       "         'AD-DIYAR': 2,\n",
       "         'Financial': 2,\n",
       "         'Hariri': 1,\n",
       "         'AL-WATAN': 2,\n",
       "         'Maronite': 1,\n",
       "         'live': 5,\n",
       "         'feeder': 2,\n",
       "         'mixed': 3,\n",
       "         'calls': 1,\n",
       "         'higher': 11,\n",
       "         'spreading': 1,\n",
       "         'evening': 7,\n",
       "         'up': 81,\n",
       "         'traded': 2,\n",
       "         'buy': 4,\n",
       "         'Q1': 2,\n",
       "         'par': 1,\n",
       "         '10-3/8': 1,\n",
       "         'preschool': 1,\n",
       "         'buyback': 1,\n",
       "         'offered': 5,\n",
       "         'future': 9,\n",
       "         'outperform': 6,\n",
       "         'estimate': 2,\n",
       "         'set': 19,\n",
       "         'Southern': 1,\n",
       "         '38-1/2': 1,\n",
       "         '212-859-1736': 1,\n",
       "         'Data': 6,\n",
       "         'net': 15,\n",
       "         'Thousands': 1,\n",
       "         'Months': 2,\n",
       "         'Ended': 2,\n",
       "         'Jul': 3,\n",
       "         'Revenue': 3,\n",
       "         'Services': 2,\n",
       "         'Operating': 1,\n",
       "         'Income': 1,\n",
       "         'Per': 3,\n",
       "         'Share': 1,\n",
       "         'Working': 2,\n",
       "         'Cash': 6,\n",
       "         'Equivalents': 1,\n",
       "         'Equity': 1,\n",
       "         'backing': 2,\n",
       "         'leading': 8,\n",
       "         'daily': 7,\n",
       "         'dated': 4,\n",
       "         'Africans': 1,\n",
       "         'enters': 3,\n",
       "         '49th': 1,\n",
       "         'rally': 1,\n",
       "         'FLNC': 1,\n",
       "         'Corsican': 1,\n",
       "         'announces': 2,\n",
       "         'BUSINESS': 3,\n",
       "         'PAGES': 2,\n",
       "         'Shutdown': 1,\n",
       "         'points': 39,\n",
       "         'shoe': 1,\n",
       "         'undercut': 1,\n",
       "         'low-wage': 1,\n",
       "         'abreast': 1,\n",
       "         'general': 7,\n",
       "         '+33': 4,\n",
       "         'Statoil': 1,\n",
       "         'Heidrun': 2,\n",
       "         'oilfield': 1,\n",
       "         'off': 37,\n",
       "         'mid-Norway': 1,\n",
       "         'plugged': 1,\n",
       "         'bpd': 4,\n",
       "         '+47': 1,\n",
       "         'surplus': 2,\n",
       "         'Finland': 5,\n",
       "         'Trade': 1,\n",
       "         'markka': 6,\n",
       "         'Jan-April': 2,\n",
       "         \"'\": 10,\n",
       "         '+3,831': 1,\n",
       "         '+3,428': 1,\n",
       "         '+12,696': 1,\n",
       "         '+15,272': 1,\n",
       "         'January-April': 1,\n",
       "         'earlier': 24,\n",
       "         'monthly': 1,\n",
       "         'trade': 7,\n",
       "         '+358': 3,\n",
       "         'raises': 2,\n",
       "         'tap': 2,\n",
       "         'AMSTERDAM': 26,\n",
       "         'Tap': 1,\n",
       "         'Amsterdam': 7,\n",
       "         '+31': 13,\n",
       "         'BONN': 20,\n",
       "         'avoid': 2,\n",
       "         'give': 3,\n",
       "         'suspect': 3,\n",
       "         'GOLF': 16,\n",
       "         'SCORES': 3,\n",
       "         'WORLD': 18,\n",
       "         'SERIES': 4,\n",
       "         'OF': 41,\n",
       "         'stated': 15,\n",
       "         'Forsbrand': 1,\n",
       "         'Mickelson': 3,\n",
       "         'Couples': 1,\n",
       "         'SOCCER': 131,\n",
       "         'BEAT': 17,\n",
       "         '2-1': 21,\n",
       "         'F.C.': 2,\n",
       "         'beat': 307,\n",
       "         'halftime': 24,\n",
       "         '1-1': 14,\n",
       "         'match': 9,\n",
       "         'Agius': 2,\n",
       "         '24th': 7,\n",
       "         'Attendance': 35,\n",
       "         '4-2': 4,\n",
       "         'qualified': 6,\n",
       "         'winners': 1,\n",
       "         'REUTER': 3,\n",
       "         'ENDS': 3,\n",
       "         'PRESCOTT': 1,\n",
       "         'YORK': 2,\n",
       "         'Stakes': 1,\n",
       "         'veteran': 6,\n",
       "         'Eveningperformance': 2,\n",
       "         '16-1': 2,\n",
       "         'Henry': 2,\n",
       "         'Candy': 2,\n",
       "         '11-4': 3,\n",
       "         'lengths': 1,\n",
       "         'Mind': 4,\n",
       "         'Pivotal': 2,\n",
       "         'sprint': 7,\n",
       "         'reluctant': 1,\n",
       "         'Twenty-five': 1,\n",
       "         'so': 12,\n",
       "         'better': 13,\n",
       "         'longer': 4,\n",
       "         'godfather': 1,\n",
       "         'Games': 4,\n",
       "         'I`m': 1,\n",
       "         'disappointed': 3,\n",
       "         'suicidal': 1,\n",
       "         'half': 22,\n",
       "         'RESULTS': 89,\n",
       "         'upwards': 1,\n",
       "         'run': 20,\n",
       "         '100-30': 1,\n",
       "         'ridden': 1,\n",
       "         '3.': 95,\n",
       "         'Eight': 1,\n",
       "         'ran': 1,\n",
       "         'Favourite': 1,\n",
       "         '7-4': 10,\n",
       "         '4th': 4,\n",
       "         '1-1/4': 1,\n",
       "         'Value': 1,\n",
       "         'winner': 1,\n",
       "         'TENNIS': 31,\n",
       "         '1996-08-21': 7,\n",
       "         'denotes': 15,\n",
       "         'seeding': 15,\n",
       "         'Naoko': 1,\n",
       "         '1-6': 12,\n",
       "         '6-4': 97,\n",
       "         '6-3': 93,\n",
       "         'Kimiko': 5,\n",
       "         'Yone': 2,\n",
       "         '7-5': 48,\n",
       "         'Shi-Ting': 1,\n",
       "         '6-2': 75,\n",
       "         'Richey': 1,\n",
       "         'Younes': 2,\n",
       "         '5-7': 14,\n",
       "         'SAD': 1,\n",
       "         'OVER': 2,\n",
       "         'premier': 14,\n",
       "         '1994-95': 2,\n",
       "         'Blackburn': 6,\n",
       "         'announced': 5,\n",
       "         'confessed': 4,\n",
       "         'get': 3,\n",
       "         'first': 37,\n",
       "         'ENGLISH': 17,\n",
       "         'COUNTY': 7,\n",
       "         'CHAMPIONSHIP': 10,\n",
       "         'Close': 5,\n",
       "         '7-73': 3,\n",
       "         'Somerset': 4,\n",
       "         '236-4': 1,\n",
       "         'Firsy': 1,\n",
       "         'Gloucestershire': 8,\n",
       "         '5-68': 2,\n",
       "         '72-0': 1,\n",
       "         '128-1': 1,\n",
       "         'v': 85,\n",
       "         'Glamorgan': 3,\n",
       "         'Leicestershire': 3,\n",
       "         'Sussex': 5,\n",
       "         '368-7': 1,\n",
       "         'Warwickshire': 5,\n",
       "         'A.': 15,\n",
       "         '52': 1,\n",
       "         'Yorkshire': 8,\n",
       "         'Lancashire': 5,\n",
       "         'ENGLAND': 13,\n",
       "         'Scoreboard': 8,\n",
       "         'Oval': 19,\n",
       "         'innings': 70,\n",
       "         'b': 74,\n",
       "         'c': 40,\n",
       "         'lb-11': 1,\n",
       "         'nb-8': 3,\n",
       "         'wickets': 10,\n",
       "         '1-64': 2,\n",
       "         '2-85': 2,\n",
       "         '3-116': 2,\n",
       "         '4-205': 2,\n",
       "         '5-248': 2,\n",
       "         '6-273': 2,\n",
       "         'date': 4,\n",
       "         'Wasim': 7,\n",
       "         '20-6-70-2': 1,\n",
       "         '6-1-17-0': 2,\n",
       "         'Pakistan': 8,\n",
       "         'IN': 48,\n",
       "         'SCOTTISH': 2,\n",
       "         'SQUAD': 3,\n",
       "         'AFTER': 2,\n",
       "         'Scottish': 4,\n",
       "         'striker': 2,\n",
       "         'late': 15,\n",
       "         'head-butting': 1,\n",
       "         'Ally': 1,\n",
       "         'scoring': 2,\n",
       "         'under-21': 3,\n",
       "         '100-2': 1,\n",
       "         'TEST': 5,\n",
       "         'CONTRACT': 1,\n",
       "         'WITH': 10,\n",
       "         'champions': 10,\n",
       "         'delighted': 2,\n",
       "         'CANADIAN': 6,\n",
       "         'OPEN': 6,\n",
       "         'TORONTO': 24,\n",
       "         'Goran': 2,\n",
       "         '3-7': 3,\n",
       "         '3': 15,\n",
       "         'Czech': 9,\n",
       "         'Todd': 6,\n",
       "         'Marc': 3,\n",
       "         '3-6': 15,\n",
       "         '7-6': 47,\n",
       "         'Cedric': 4,\n",
       "         'Alberto': 3,\n",
       "         '6-1': 48,\n",
       "         'Petr': 2,\n",
       "         '7-1': 5,\n",
       "         'Andrea': 2,\n",
       "         'fast': 5,\n",
       "         'replaces': 3,\n",
       "         'CUP': 5,\n",
       "         'WINNERS': 1,\n",
       "         'TIRANA': 5,\n",
       "         'Winners': 5,\n",
       "         'round': 26,\n",
       "         'Flamurtari': 1,\n",
       "         '0-0': 22,\n",
       "         '50th': 2,\n",
       "         '3-0': 8,\n",
       "         'aggregate': 10,\n",
       "         '55th': 2,\n",
       "         '62nd': 1,\n",
       "         '63rd': 1,\n",
       "         'Ruch': 2,\n",
       "         '5-1': 6,\n",
       "         'Sion': 3,\n",
       "         'win': 31,\n",
       "         'agrregate': 1,\n",
       "         'Aggregate': 6,\n",
       "         'score': 1,\n",
       "         'Nyva': 1,\n",
       "         'away': 14,\n",
       "         'rule': 3,\n",
       "         'Mons': 1,\n",
       "         '10th': 14,\n",
       "         'Shelbourne': 1,\n",
       "         'Mark': 2,\n",
       "         '5th': 4,\n",
       "         'Scorer': 3,\n",
       "         'Ilian': 1,\n",
       "         '4-3': 3,\n",
       "         'Vaduz': 1,\n",
       "         'Agrins': 1,\n",
       "         'Zarins': 1,\n",
       "         '2-2': 11,\n",
       "         'US': 2,\n",
       "         '78th': 1,\n",
       "         'Torshavn': 1,\n",
       "         'Dynamo': 1,\n",
       "         '4-0': 2,\n",
       "         '26th': 6,\n",
       "         '19th': 4,\n",
       "         'Hearts': 5,\n",
       "         'Red': 2,\n",
       "         '3-3': 1,\n",
       "         'Constructorul': 1,\n",
       "         'Mypa-47': 1,\n",
       "         'Hungary': 1,\n",
       "         '2-0': 20,\n",
       "         'Add': 6,\n",
       "         'Rishon': 1,\n",
       "         'Constructorol': 1,\n",
       "         '42nd': 4,\n",
       "         '87th': 1,\n",
       "         '1,500': 1,\n",
       "         'BUDAPEST': 5,\n",
       "         '1-0': 33,\n",
       "         'Ferencvaros': 3,\n",
       "         'tie': 3,\n",
       "         'played': 68,\n",
       "         'Gothenburg': 1,\n",
       "         '4-1': 6,\n",
       "         '15th': 7,\n",
       "         'midweek': 3,\n",
       "         'Goias': 3,\n",
       "         'da': 3,\n",
       "         'NEWCOMBE': 1,\n",
       "         'PONDERS': 1,\n",
       "         'HIS': 1,\n",
       "         'DAVIS': 1,\n",
       "         'FUTURE': 1,\n",
       "         'Australian': 23,\n",
       "         'lose': 1,\n",
       "         'one': 15,\n",
       "         'face': 5,\n",
       "         'Newcombe': 2,\n",
       "         'as': 19,\n",
       "         'doubles': 7,\n",
       "         'semifinalist': 2,\n",
       "         'Olympic': 27,\n",
       "         'absolute': 1,\n",
       "         'toughest': 1,\n",
       "         'ago': 7,\n",
       "         'Open': 4,\n",
       "         '15-3': 3,\n",
       "         '15-7': 6,\n",
       "         '15-2': 4,\n",
       "         '17-14': 3,\n",
       "         'Netherlands': 6,\n",
       "         '18-14': 1,\n",
       "         'Pang': 1,\n",
       "         '15-6': 11,\n",
       "         '6-15': 1,\n",
       "         '3/4': 3,\n",
       "         'Hu': 2,\n",
       "         '5-15': 1,\n",
       "         '18-15': 1,\n",
       "         'Fung': 1,\n",
       "         '15-8': 5,\n",
       "         '15-12': 9,\n",
       "         'Women': 52,\n",
       "         '2nd': 2,\n",
       "         'Wang': 3,\n",
       "         'Margit': 1,\n",
       "         '11-6': 4,\n",
       "         '11-2': 3,\n",
       "         'Meluawati': 1,\n",
       "         '11-1': 1,\n",
       "         '6-11': 1,\n",
       "         '11-7': 2,\n",
       "         '11-3': 1,\n",
       "         'Zeng': 1,\n",
       "         'Christine': 1,\n",
       "         '10-12': 1,\n",
       "         'Zhang': 2,\n",
       "         'REVISED': 1,\n",
       "         'MEN': 1,\n",
       "         'DRAW': 5,\n",
       "         'NEW': 78,\n",
       "         'draw': 5,\n",
       "         'Draw': 1,\n",
       "         'Sampras': 5,\n",
       "         'vs.': 57,\n",
       "         'Adrian': 10,\n",
       "         'qualifier': 9,\n",
       "         'vs': 44,\n",
       "         'Qualifier': 9,\n",
       "         '------------------------': 7,\n",
       "         'Christian': 7,\n",
       "         'Sjeng': 2,\n",
       "         'Forget': 2,\n",
       "         'Yevgeny': 1,\n",
       "         'Costa': 1,\n",
       "         'Younnes': 1,\n",
       "         'Mats': 3,\n",
       "         'Andre': 3,\n",
       "         'Knowles': 3,\n",
       "         'Bahamas': 6,\n",
       "         'Vince': 1,\n",
       "         'BALTIMORE': 18,\n",
       "         'Mariners': 9,\n",
       "         'irregular': 3,\n",
       "         'observed': 2,\n",
       "         'Angels': 2,\n",
       "         'skipper': 1,\n",
       "         'off-season': 1,\n",
       "         'replacing': 1,\n",
       "         'Yankees': 11,\n",
       "         'GAMES': 14,\n",
       "         'tabulate': 28,\n",
       "         'won': 28,\n",
       "         'lost': 53,\n",
       "         '.576': 1,\n",
       "         '.536': 3,\n",
       "         '.496': 9,\n",
       "         '.457': 5,\n",
       "         'DETROIT': 9,\n",
       "         '.349': 1,\n",
       "         '.598': 4,\n",
       "         '.539': 2,\n",
       "         '.500': 5,\n",
       "         'MILWAUKEE': 2,\n",
       "         '.469': 5,\n",
       "         '.453': 1,\n",
       "         '.575': 2,\n",
       "         '.512': 4,\n",
       "         'OAKLAND': 15,\n",
       "         '.481': 2,\n",
       "         '.460': 1,\n",
       "         'AUGUST': 10,\n",
       "         'SCHEDULE': 14,\n",
       "         'SEATTLE': 1,\n",
       "         'AT': 5,\n",
       "         'KANSAS': 6,\n",
       "         'TEXAS': 6,\n",
       "         '.632': 1,\n",
       "         '.461': 4,\n",
       "         '.409': 1,\n",
       "         '.535': 3,\n",
       "         'ST': 9,\n",
       "         'LOUIS': 9,\n",
       "         '.532': 2,\n",
       "         '.504': 6,\n",
       "         'PITTSBURGH': 12,\n",
       "         '.421': 1,\n",
       "         '.543': 6,\n",
       "         '.524': 1,\n",
       "         '.435': 3,\n",
       "         'FRANCISCO': 7,\n",
       "         'Baseball': 7,\n",
       "         'INDIANS': 1,\n",
       "         '10-8': 2,\n",
       "         'Indians': 4,\n",
       "         'rubber': 4,\n",
       "         'Julian': 2,\n",
       "         '4-7': 1,\n",
       "         'put': 10,\n",
       "         'double': 8,\n",
       "         'Western': 15,\n",
       "         'winning': 2,\n",
       "         'Cleveland': 1,\n",
       "         'single': 4,\n",
       "         'save': 1,\n",
       "         'scoreless': 5,\n",
       "         'fifth-inning': 2,\n",
       "         'seventh': 2,\n",
       "         'surging': 1,\n",
       "         '10-5': 1,\n",
       "         'fifth': 8,\n",
       "         '5-5': 2,\n",
       "         '21st': 5,\n",
       "         'starter': 3,\n",
       "         'Sterling': 1,\n",
       "         '12-6': 1,\n",
       "         'cut': 8,\n",
       "         '8-10': 1,\n",
       "         'dropped': 2,\n",
       "         '22nd': 2,\n",
       "         'major-league': 4,\n",
       "         'snapped': 4,\n",
       "         'eighth-inning': 1,\n",
       "         '.367': 1,\n",
       "         'loaded': 1,\n",
       "         '33-for-90': 1,\n",
       "         'stole': 2,\n",
       "         'collected': 1,\n",
       "         '116th': 1,\n",
       "         'drove': 8,\n",
       "         'Athletics': 1,\n",
       "         'Tigers': 5,\n",
       "         'Sox': 6,\n",
       "         'league-best': 1,\n",
       "         'Jays': 3,\n",
       "         'straight': 7,\n",
       "         'earned': 2,\n",
       "         'ERA': 1,\n",
       "         'Brewers': 4,\n",
       "         'Jaha': 1,\n",
       "         'COCU': 1,\n",
       "         'scored': 6,\n",
       "         'Dutch': 31,\n",
       "         '54th': 3,\n",
       "         'top': 5,\n",
       "         'DUTCH': 14,\n",
       "         'Summary': 11,\n",
       "         '11th': 13,\n",
       "         'Nilis': 1,\n",
       "         'Halftime': 24,\n",
       "         '1-2': 5,\n",
       "         'Galatasaray': 1,\n",
       "         'recalled': 2,\n",
       "         'Knup': 2,\n",
       "         'Squad': 7,\n",
       "         'Grasshoppers': 5,\n",
       "         'Stuttgart': 4,\n",
       "         'IT': 2,\n",
       "         'A': 15,\n",
       "         'RECORD': 5,\n",
       "         'ON': 7,\n",
       "         'prix': 3,\n",
       "         'a': 1,\n",
       "         'GERMAN': 11,\n",
       "         'Leading': 30,\n",
       "         'Eamonn': 2,\n",
       "         'Anglert': 1,\n",
       "         'Chalmers': 1,\n",
       "         'Cooper': 1,\n",
       "         'Canizares': 2,\n",
       "         'Spence': 1,\n",
       "         'UEFA': 1,\n",
       "         'REWARDS': 1,\n",
       "         'THREE': 1,\n",
       "         'COUNTRIES': 1,\n",
       "         'Norway': 4,\n",
       "         '1997-98': 2,\n",
       "         'Play': 3,\n",
       "         'rankings': 1,\n",
       "         '1995-96': 1,\n",
       "         '1': 1,\n",
       "         '2.': 54,\n",
       "         'Islands': 2,\n",
       "         '5.': 26,\n",
       "         'Wales': 1,\n",
       "         '7.': 18,\n",
       "         '16.': 3,\n",
       "         'Yugoslavia': 1,\n",
       "         '19.': 2,\n",
       "         '21.': 2,\n",
       "         '25.': 1,\n",
       "         '26.': 2,\n",
       "         '30.': 1,\n",
       "         '32.': 1,\n",
       "         '33.': 2,\n",
       "         '35.': 1,\n",
       "         '36.': 1,\n",
       "         '37.': 2,\n",
       "         '39.': 1,\n",
       "         '45.': 1,\n",
       "         'police': 104,\n",
       "         'commandos': 3,\n",
       "         'sniffer': 2,\n",
       "         \"'s\": 13,\n",
       "         'warm-up': 1,\n",
       "         'Ricky': 1,\n",
       "         'youth': 1,\n",
       "         'impressed': 1,\n",
       "         'ROMANIAN': 1,\n",
       "         'DIES': 2,\n",
       "         'CRASH': 1,\n",
       "         'collided': 5,\n",
       "         \"o'clock\": 2,\n",
       "         'Romanian': 2,\n",
       "         'Sofia': 2,\n",
       "         '359-2-84561': 2,\n",
       "         'CONTENTS': 4,\n",
       "         'OJ': 2,\n",
       "         '*': 30,\n",
       "         'Note': 6,\n",
       "         'printed': 3,\n",
       "         'refunds': 1,\n",
       "         'No': 12,\n",
       "         'compensatory': 1,\n",
       "         'DOCUMENT': 4,\n",
       "         'related': 2,\n",
       "         'disputed': 2,\n",
       "         'distrct': 1,\n",
       "         'disputes': 1,\n",
       "         '312-408-8787': 3,\n",
       "         'ended': 9,\n",
       "         'sees': 2,\n",
       "         'Q2': 2,\n",
       "         'Best': 5,\n",
       "         'posted': 1,\n",
       "         'emerged': 2,\n",
       "         '3-1/2': 2,\n",
       "         'second': 21,\n",
       "         'bowel': 1,\n",
       "         'study': 1,\n",
       "         '1996-08-23': 73,\n",
       "         'measles': 1,\n",
       "         'pregnant': 1,\n",
       "         'debilitating': 1,\n",
       "         'weight': 1,\n",
       "         'Exposure': 1,\n",
       "         'Most': 2,\n",
       "         'rubella': 1,\n",
       "         'have': 16,\n",
       "         'TRENDS': 1,\n",
       "         'ENQUIRY': 1,\n",
       "         'total': 3,\n",
       "         'finished': 1,\n",
       "         '+17': 2,\n",
       "         '+19': 1,\n",
       "         '+25': 1,\n",
       "         '+22': 2,\n",
       "         '+12': 1,\n",
       "         '+16': 1,\n",
       "         '+6': 2,\n",
       "         '+4': 4,\n",
       "         'NOTES': 3,\n",
       "         'manufactured': 1,\n",
       "         'Secondhand': 1,\n",
       "         'built': 5,\n",
       "         'Greek': 7,\n",
       "         'Garlic': 2,\n",
       "         'finds': 1,\n",
       "         'flawed': 1,\n",
       "         'garlic': 5,\n",
       "         'involved': 5,\n",
       "         'dried': 1,\n",
       "         'low-fat': 1,\n",
       "         'beforehand': 1,\n",
       "         'short': 14,\n",
       "         'British': 21,\n",
       "         'Kwai': 1,\n",
       "         'Caribbean': 2,\n",
       "         'much': 20,\n",
       "         'hills': 1,\n",
       "         'Plymouth': 4,\n",
       "         'Overseas': 1,\n",
       "         'Montserratians': 1,\n",
       "         'Tennis': 3,\n",
       "         'Philippoussis': 4,\n",
       "         'top-ranked': 1,\n",
       "         'title': 3,\n",
       "         'Wimbledon': 6,\n",
       "         'Bumping': 1,\n",
       "         'semis': 1,\n",
       "         'American': 20,\n",
       "         'Austrian': 2,\n",
       "         'yet': 4,\n",
       "         'unfortunate': 1,\n",
       "         'first-round': 1,\n",
       "         'eighth': 1,\n",
       "         'Swede': 4,\n",
       "         'rib': 2,\n",
       "         'Graf': 3,\n",
       "         'South': 9,\n",
       "         'French': 20,\n",
       "         '16th': 4,\n",
       "         'Martinez': 1,\n",
       "         'Sydney': 15,\n",
       "         '61-2': 13,\n",
       "         '9373-1800': 9,\n",
       "         'RTRS': 12,\n",
       "         'Muster': 1,\n",
       "         'upset': 3,\n",
       "         'Canadian': 6,\n",
       "         'attacking': 1,\n",
       "         'tactic': 1,\n",
       "         'worked': 2,\n",
       "         'second-round': 2,\n",
       "         '6-7(3-7': 1,\n",
       "         'seeded': 13,\n",
       "         'States': 53,\n",
       "         'unseeded': 2,\n",
       "         '5-6': 1,\n",
       "         '8-6': 2,\n",
       "         '13th': 6,\n",
       "         'fortunate': 1,\n",
       "         'lanky': 1,\n",
       "         'hit': 6,\n",
       "         'whole': 2,\n",
       "         'timing': 1,\n",
       "         'like': 5,\n",
       "         'maybe': 1,\n",
       "         '2-5': 2,\n",
       "         'Tillstrom': 1,\n",
       "         'Something': 1,\n",
       "         'little': 7,\n",
       "         'calm': 1,\n",
       "         'pleased': 2,\n",
       "         'fought': 1,\n",
       "         'serves': 3,\n",
       "         'hoped': 1,\n",
       "         'tight': 8,\n",
       "         '5-all': 1,\n",
       "         'back': 21,\n",
       "         'Soccer': 6,\n",
       "         'pro-soccer': 2,\n",
       "         'SEOUL': 11,\n",
       "         'Anyang': 4,\n",
       "         'Puchon': 5,\n",
       "         'drawn': 49,\n",
       "         'F': 9,\n",
       "         'Chonan': 5,\n",
       "         'Suwon': 4,\n",
       "         'Pohang': 4,\n",
       "         'Pusan': 4,\n",
       "         'Senegal': 3,\n",
       "         'DAKAR': 2,\n",
       "         'southeast': 3,\n",
       "         'Liberia': 10,\n",
       "         'ECOMOG': 1,\n",
       "         'Nigerian': 3,\n",
       "         'Malu': 1,\n",
       "         'replaced': 1,\n",
       "         'challenging': 1,\n",
       "         'Nations': 15,\n",
       "         'military': 8,\n",
       "         'representative': 4,\n",
       "         'West': 2,\n",
       "         'African': 17,\n",
       "         'broadcast': 1,\n",
       "         'clear': 8,\n",
       "         'Koranic': 1,\n",
       "         'km': 13,\n",
       "         'early': 7,\n",
       "         'JOHANNESBURG': 12,\n",
       "         'washed': 1,\n",
       "         'flung': 1,\n",
       "         'Rottweiler': 1,\n",
       "         'rottweiler': 1,\n",
       "         'death': 5,\n",
       "         'attacked': 2,\n",
       "         'bloody': 2,\n",
       "         'arrived': 7,\n",
       "         'Dogs': 1,\n",
       "         'fierce': 3,\n",
       "         'enough': 7,\n",
       "         'INDICATORS': 4,\n",
       "         'updated': 2,\n",
       "         '+0.4m': 1,\n",
       "         'm': 6,\n",
       "         '+0.7': 1,\n",
       "         'm;+21.5yr': 1,\n",
       "         '+1.7;+22.0': 1,\n",
       "         '+7.3;-3.6': 1,\n",
       "         'MIT': 1,\n",
       "         'Jan-June': 1,\n",
       "         'Jan-May': 1,\n",
       "         'HUF': 1,\n",
       "         'Jan-July': 1,\n",
       "         'T-bill': 1,\n",
       "         'Government': 5,\n",
       "         '2-yr': 1,\n",
       "         'BA1': 1,\n",
       "         'Investors': 5,\n",
       "         'Budapest': 4,\n",
       "         'Fifty': 1,\n",
       "         'MOSCOW': 17,\n",
       "         'separatist': 11,\n",
       "         'continued': 9,\n",
       "         'Interfax': 11,\n",
       "         'about': 37,\n",
       "         'interior': 6,\n",
       "         'reconaisance': 1,\n",
       "         'rebel': 13,\n",
       "         'made': 8,\n",
       "         'GMT': 11,\n",
       "         'servicemen': 1,\n",
       "         'blackmailed': 1,\n",
       "         'explained': 3,\n",
       "         'zlotys': 1,\n",
       "         'Interviewed': 1,\n",
       "         'CNB-120': 2,\n",
       "         'ten': 2,\n",
       "         'Prague': 2,\n",
       "         '42-2-2423-0003': 2,\n",
       "         'Russians': 2,\n",
       "         'sign': 2,\n",
       "         'supremo': 1,\n",
       "         'chief-of-staff': 4,\n",
       "         'aimed': 2,\n",
       "         'renewed': 3,\n",
       "         'south': 16,\n",
       "         'Itar-Tass': 6,\n",
       "         'Lebed': 18,\n",
       "         'detailed': 1,\n",
       "         'Yeltsin': 7,\n",
       "         'brief': 2,\n",
       "         'nominee': 1,\n",
       "         'DIGEST': 8,\n",
       "         'SARAJEVO': 4,\n",
       "         'OSLOBODJENJE': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.Counter(wordslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_dataset_nltk = []\n",
    "# dataset_for_tagging_nltk =[]\n",
    "# path_conll_nltk_tags ='/Users/hernando/Desktop/NLP/assignment3/error_propagation 2/data/conll_train_cap.txt'\n",
    "# with (open(path_conll_nltk_tags)) as f:\n",
    "#     csv_file = csv.reader(f, delimiter='\\t', quotechar=None)\n",
    "\n",
    "#     # For each sentence, we want to store the following information:\n",
    "#     # words, pos-tags (from the MBT Tagger), ne-labels\n",
    "#     words, mbt_tags, labels = [], [], []\n",
    "\n",
    "#     for idx, row in enumerate(csv_file):\n",
    "#         # Keep all lines unchanged here\n",
    "#         original_dataset_nltk.append(row)\n",
    "\n",
    "#         # Empty rows indicate sentence boundaries. We add the collected information to the dataset\n",
    "#         if len(row) == 0:\n",
    "\n",
    "#             # Make sure that everything has the same length\n",
    "#             assert len(words) == len(mbt_tags) == len(labels)\n",
    "\n",
    "#             # Add the information for the current sentence to the dataset\n",
    "#             # Each sentence is a tuple of words, tags, and labels\n",
    "#             dataset_for_tagging_nltk.append((words, mbt_tags, labels))\n",
    "\n",
    "#             # Re-initialize the variables\n",
    "#             words, mbt_tags, labels = [], [], []\n",
    "\n",
    "#         else:\n",
    "#             word, previous, casing, pos, chunk, label = row\n",
    "#             word = str(row[0])\n",
    "#             mbt_tag = str(row[3])\n",
    "#             label = str(row[5])\n",
    "\n",
    "#             words.append(word)\n",
    "#             mbt_tags.append(pos)\n",
    "#             labels.append(label)\n",
    "\n",
    "# # The last empty line is not processed by the reader, so we need to add the last sentence here.\n",
    "# if not len(words) == 0:\n",
    "#     dataset_for_tagging_nltk.append((words, mbt_tags, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differnet_pos = 0\n",
    "# differnet_label = 0\n",
    "# different_both = 0\n",
    "# NNP_=0\n",
    "# NNP_nltk=0\n",
    "# number_different = []\n",
    "# for index,sentence in enumerate(dataset_for_tagging):\n",
    "#     # Split the tuple into the different categories\n",
    "#     _, mbt_tags, labels = dataset_for_tagging[index]\n",
    "#     _, mbt_tags_nltk, labels_nltk = dataset_for_tagging_nltk[index]\n",
    "# #     print(mbt_tags,mbt_tags_nltk)\n",
    "#     for i in range(len(mbt_tags)):\n",
    "#         if mbt_tags[i] != mbt_tags_nltk[i]:\n",
    "#             print('1')\n",
    "#             number_different.append((mbt_tags[i],mbt_tags_nltk[i]))\n",
    "# #         if labels[i] != labels_nltk[i]:\n",
    "# #             differnet_label+=1\n",
    "# #         if mbt_tags[i] != mbt_tags_nltk[i] and labels[i] != labels_nltk[i]:\n",
    "# #             different_both +=1\n",
    "#         if mbt_tags[i]=='NNP':\n",
    "#             NNP_+=1\n",
    "#         if mbt_tags_nltk[i]=='NNP':\n",
    "#             NNP_nltk+=1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as cs\n",
    "import sklearn\n",
    "import csv\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = '/Users/hernando/Desktop/NLP/assignment3/error_propagation 2/data/conll_train_cap_nltktags.txt'\n",
    "testfile = '/Users/hernando/Desktop/NLP/assignment3/error_propagation 2/data/conll_test_cap_nltktags.txt'\n",
    "\n",
    "# the functions with multiple features and analysis\n",
    "\n",
    "#defines the column in which each feature is located (note: you can also define headers and use csv.DictReader)\n",
    "feature_to_index = {'Token': 0, 'Prevtoken': 1, 'Cap': 2, 'Pos': 3, 'Chunklabel': 4}\n",
    "\n",
    "def extract_features_token_only_and_labels(conllfile):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "    #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "    #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "        if len(row) == 6:\n",
    "            #structuring feature value pairs as key-value pairs in a dictionary\n",
    "            #the first column in the conll file represents tokens\n",
    "            feature_value = {'Token': row[0]}\n",
    "            features.append(feature_value)\n",
    "            #The last column provides the gold label (= the correct answer). \n",
    "            labels.append(row[-1])\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "def create_vectorizer_and_classifier(features, labels):\n",
    "    '''\n",
    "    Function that takes feature-value pairs and gold labels as input and trains a logistic regression classifier\n",
    "    \n",
    "    :param features: feature-value pairs\n",
    "    :param labels: gold labels\n",
    "    :type features: a list of dictionaries\n",
    "    :type labels: a list of strings\n",
    "    \n",
    "    :return lr_classifier: a trained LogisticRegression classifier\n",
    "    :return vec: a DictVectorizer to which the feature values are fitted. \n",
    "    '''\n",
    "    \n",
    "    vec = DictVectorizer()\n",
    "    #fit creates a mapping between observed feature values and dimensions in a one-hot vector, transform represents the current values as a vector \n",
    "    tokens_vectorized = vec.fit_transform(features)\n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(tokens_vectorized, labels)\n",
    "    \n",
    "    return lr_classifier, vec\n",
    "\n",
    "def get_predicted_and_gold_labels_token_only(testfile, vectorizer, classifier):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    sparse_feature_reps, goldlabels = extract_features_token_only_and_labels(testfile)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(sparse_feature_reps)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "def print_confusion_matrix(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out a confusion matrix\n",
    "    \n",
    "    :param predictions: predicted labels\n",
    "    :param goldlabels: gold standard labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #based on example from https://datatofish.com/confusion-matrix-python/ \n",
    "    data = {'Gold':    goldlabels, 'Predicted': predictions    }\n",
    "    df = pd.DataFrame(data, columns=['Gold','Predicted'])\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df['Gold'], df['Predicted'], rownames=['Gold'], colnames=['Predicted'])\n",
    "    print (confusion_matrix)\n",
    "\n",
    "\n",
    "def print_precision_recall_fscore(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out precision, recall and f-score\n",
    "    \n",
    "    :param predictions: predicted output by classifier\n",
    "    :param goldlabels: original gold labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    precision = metrics.precision_score(y_true=goldlabels,\n",
    "                        y_pred=predictions,average='macro')\n",
    "\n",
    "    recall = metrics.recall_score(y_true=goldlabels,\n",
    "                     y_pred=predictions,average='macro')\n",
    "\n",
    "\n",
    "    fscore = metrics.f1_score(y_true=goldlabels,\n",
    "                 y_pred=predictions,average='macro')\n",
    "\n",
    "    print('P:', precision, 'R:', recall, 'F1:', fscore)\n",
    "\n",
    "def extract_features_and_gold_labels(conllfile, selected_features):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "    #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "    #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "        if len(row) == 6:\n",
    "            #structuring feature value pairs as key-value pairs in a dictionary\n",
    "            #the first column in the conll file represents tokens\n",
    "            feature_value = {}\n",
    "            for feature_name in selected_features:\n",
    "                row_index = feature_to_index.get(feature_name)\n",
    "                feature_value[feature_name] = row[row_index]\n",
    "            features.append(feature_value)\n",
    "            #The last column provides the gold label (= the correct answer). \n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "\n",
    "def get_predicted_and_gold_labels(testfile, vectorizer, classifier, selected_features):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    features, goldlabels = extract_features_and_gold_labels(testfile, selected_features)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(features)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1358      10    134     77      1       1      6     41     40\n",
      "B-MISC        27     449     56     48      0       1      6     26     89\n",
      "B-ORG        188      44   1021    175      1       1     25     63    143\n",
      "B-PER         75      11     43   1276      0       0      1    140     71\n",
      "I-LOC          4       0      0      0    158       4     36     32     23\n",
      "I-MISC         1       8      0      3      2     130     10     25     37\n",
      "I-ORG         24       7      6     11     46       8    513    112    108\n",
      "I-PER          1       0      0     14      1       0     14   1105     21\n",
      "O             34      34     78    107      3      38     35    106  38119\n"
     ]
    }
   ],
   "source": [
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "all_features = ['Token','Prevtoken','Cap','Pos','Chunklabel']\n",
    "\n",
    "sparse_feature_reps, labels = extract_features_and_gold_labels(trainfile, all_features)\n",
    "#we can use the same function as before for creating the classifier and vectorizer\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(sparse_feature_reps, labels)\n",
    "#when applying our model to new data, we need to use the same features\n",
    "predictions, goldlabels = get_predicted_and_gold_labels(testfile, vectorizer, lr_classifier, all_features)\n",
    "print_confusion_matrix(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.7783661674293809 R: 0.7370182212511502 F1: 0.7504892659594187\n"
     ]
    }
   ],
   "source": [
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/hernando/Desktop/NLP/Assignment2/NLP_tech_distributional_semantics/models/GoogleNews-vectors-negative300.bin.gz', binary=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier....\n",
      "Running evaluation...\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1432      31    150     21      3       2      9      4     16\n",
      "B-MISC        23     512     54     25      1       6      4      1     76\n",
      "B-ORG        236      62   1175     78      1       2     24      4     79\n",
      "B-PER         21      16     54   1452      0       2      8     23     41\n",
      "I-LOC          5       1      3      1    170      10     42      9     16\n",
      "I-MISC         1       7      1      3      4     132     23      2     43\n",
      "I-ORG         16      14     21      3     61      26    566     30     98\n",
      "I-PER          0       9      2     33      9       1     30   1055     17\n",
      "O             22      59     87     48     15      65     98      5  38155\n",
      "P: 0.6724982682447125 R: 0.6344319254956768 F1: 0.6517226942239397\n",
      "Extracting dense features...\n",
      "Training classifier...\n",
      "Running evaluation...\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1432      31    150     21      3       2      9      4     16\n",
      "B-MISC        23     512     54     25      1       6      4      1     76\n",
      "B-ORG        236      62   1175     78      1       2     24      4     79\n",
      "B-PER         21      16     54   1452      0       2      8     23     41\n",
      "I-LOC          5       1      3      1    170      10     42      9     16\n",
      "I-MISC         1       7      1      3      4     132     23      2     43\n",
      "I-ORG         16      14     21      3     61      26    566     30     98\n",
      "I-PER          0       9      2     33      9       1     30   1055     17\n",
      "O             22      59     87     48     15      65     98      5  38155\n",
      "P: 0.7706433909643632 R: 0.7554149229938387 F1: 0.7620927927958481\n",
      "Extracting Features...\n",
      "Training classifier....\n",
      "Running the evaluation...\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1432      31    150     21      3       2      9      4     16\n",
      "B-MISC        23     512     54     25      1       6      4      1     76\n",
      "B-ORG        236      62   1175     78      1       2     24      4     79\n",
      "B-PER         21      16     54   1452      0       2      8     23     41\n",
      "I-LOC          5       1      3      1    170      10     42      9     16\n",
      "I-MISC         1       7      1      3      4     132     23      2     43\n",
      "I-ORG         16      14     21      3     61      26    567     29     98\n",
      "I-PER          0       9      2     33      9       1     30   1055     17\n",
      "O             22      59     87     48     15      65     98      5  38155\n",
      "P: 0.7749521200016916 R: 0.7830149492936358 F1: 0.7784767976956896\n"
     ]
    }
   ],
   "source": [
    "trainfile = '/Users/hernando/Desktop/NLP/assignment3/error_propagation 2/data/conll_train_cap_nltktags.txt'\n",
    "testfile = '/Users/hernando/Desktop/NLP/assignment3/error_propagation 2/data/conll_test_cap_nltktags.txt'\n",
    "\n",
    "def extract_embeddings_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 6:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "\n",
    "def create_classifier(features, labels):\n",
    "    '''\n",
    "    Function that creates classifier from features represented as vectors and gold labels\n",
    "    \n",
    "    :param features: list of vector representations of tokens\n",
    "    :param labels: list of gold labels\n",
    "    :type features: list of vectors\n",
    "    :type labels: list of strings\n",
    "    \n",
    "    :returns trained logistic regression classifier\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(features, labels)\n",
    "    \n",
    "    return lr_classifier\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features and gold labels from test data and runs a classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    dense_feature_representations, labels = extract_embeddings_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(dense_feature_representations)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "print('Extracting dense features...')\n",
    "dense_feature_representations, labels = extract_embeddings_as_features_and_gold(trainfile,word_embedding_model)\n",
    "print('Training classifier....')\n",
    "classifier = create_classifier(dense_feature_representations, labels)\n",
    "print('Running evaluation...')\n",
    "predicted, gold = label_data_using_word_embeddings(testfile, word_embedding_model, classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predicted, gold)\n",
    "\n",
    "def extract_embeddings_of_current_and_preceding_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 6:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector1 = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector1 = [0]*300\n",
    "            if row[1] in word_embedding_model:\n",
    "                vector2 = word_embedding_model[row[1]]\n",
    "            else:\n",
    "                vector2 = [0]*300\n",
    "            features.append(np.concatenate((vector1,vector2)))\n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features (of current and preceding token) and gold labels from test data and runs a trained classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(features)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "print('Extracting dense features...')\n",
    "features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(trainfile,word_embedding_model)\n",
    "print('Training classifier...')\n",
    "#we can use the same function as for just the tokens itself\n",
    "classifier = create_classifier(features, labels)\n",
    "print('Running evaluation...')\n",
    "predicted, gold = label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predicted, gold)\n",
    "\n",
    "\n",
    "def extract_word_embedding(token, word_embedding_model):\n",
    "    '''\n",
    "    Function that returns the word embedding for a given token out of a distributional semantic model and a 300-dimension vector of 0s otherwise\n",
    "    \n",
    "    :param token: the token\n",
    "    :param word_embedding_model: the distributional semantic model\n",
    "    :type token: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :returns a vector representation of the token\n",
    "    '''\n",
    "    if token in word_embedding_model:\n",
    "        vector = word_embedding_model[token]\n",
    "    else:\n",
    "        vector = [0]*300\n",
    "    return vector\n",
    "\n",
    "\n",
    "def extract_feature_values(row, selected_features):\n",
    "    '''\n",
    "    Function that extracts feature value pairs from row\n",
    "    \n",
    "    :param row: row from conll file\n",
    "    :param selected_features: list of selected features\n",
    "    :type row: string\n",
    "    :type selected_features: list of strings\n",
    "    \n",
    "    :returns: dictionary of feature value pairs\n",
    "    '''\n",
    "    feature_values = {}\n",
    "    for feature_name in selected_features:\n",
    "        r_index = feature_to_index.get(feature_name)\n",
    "        feature_values[feature_name] = row[r_index]\n",
    "        \n",
    "    return feature_values\n",
    "    \n",
    "    \n",
    "def create_vectorizer_traditional_features(feature_values):\n",
    "    '''\n",
    "    Function that creates vectorizer for set of feature values\n",
    "    \n",
    "    :param feature_values: list of dictionaries containing feature-value pairs\n",
    "    :type feature_values: list of dictionairies (key and values are strings)\n",
    "    \n",
    "    :returns: vectorizer with feature values fitted\n",
    "    '''\n",
    "    vectorizer = DictVectorizer()\n",
    "    vectorizer.fit(feature_values)\n",
    "    \n",
    "    return vectorizer\n",
    "        \n",
    "    \n",
    "def combine_sparse_and_dense_features(dense_vectors, sparse_features):\n",
    "    '''\n",
    "    Function that takes sparse and dense feature representations and appends their vector representation\n",
    "    \n",
    "    :param dense_vectors: list of dense vector representations\n",
    "    :param sparse_features: list of sparse vector representations\n",
    "    :type dense_vector: list of arrays\n",
    "    :type sparse_features: list of lists\n",
    "    \n",
    "    :returns: list of arrays in which sparse and dense vectors are concatenated\n",
    "    '''\n",
    "    \n",
    "    combined_vectors = []\n",
    "    sparse_vectors = np.array(sparse_features.toarray())\n",
    "    \n",
    "    for index, vector in enumerate(sparse_vectors):\n",
    "        combined_vector = np.concatenate((vector,dense_vectors[index]))\n",
    "        combined_vectors.append(combined_vector)\n",
    "    return combined_vectors\n",
    "    \n",
    "\n",
    "def extract_traditional_features_and_embeddings_plus_gold_labels(conllfile, word_embedding_model, vectorizer=None):\n",
    "    '''\n",
    "    Function that extracts traditional features as well as embeddings and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    dense_vectors = []\n",
    "    traditional_features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 6:\n",
    "            token_vector = extract_word_embedding(row[0], word_embedding_model)\n",
    "            pt_vector = extract_word_embedding(row[1], word_embedding_model)\n",
    "            dense_vectors.append(np.concatenate((token_vector,pt_vector)))\n",
    "            #mixing very sparse representations (for one-hot tokens) and dense representations is a bad idea\n",
    "            #we thus only use other features with limited values\n",
    "            other_features = extract_feature_values(row, ['Cap','Pos','Chunklabel'])\n",
    "            traditional_features.append(other_features)\n",
    "            #adding gold label to labels\n",
    "            labels.append(row[-1])\n",
    "            \n",
    "    #create vector representation of traditional features\n",
    "    if vectorizer is None:\n",
    "        #creates vectorizer that provides mapping (only if not created earlier)\n",
    "        vectorizer = create_vectorizer_traditional_features(traditional_features)\n",
    "    sparse_features = vectorizer.transform(traditional_features)\n",
    "    combined_vectors = combine_sparse_and_dense_features(dense_vectors, sparse_features)\n",
    "    \n",
    "    return combined_vectors, vectorizer, labels\n",
    "\n",
    "def label_data_with_combined_features(testfile, classifier, vectorizer, word_embedding_model):\n",
    "    '''\n",
    "    Function that labels data with model using both sparse and dense features\n",
    "    '''\n",
    "    feature_vectors, vectorizer, goldlabels = extract_traditional_features_and_embeddings_plus_gold_labels(testfile, word_embedding_model, vectorizer)\n",
    "    predictions = classifier.predict(feature_vectors)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "\n",
    "print('Extracting Features...')\n",
    "feature_vectors, vectorizer, gold_labels = extract_traditional_features_and_embeddings_plus_gold_labels(trainfile, word_embedding_model)\n",
    "print('Training classifier....')\n",
    "lr_classifier = create_classifier(feature_vectors, gold_labels)\n",
    "print('Running the evaluation...')\n",
    "predictions, goldlabels = label_data_with_combined_features(testfile, lr_classifier, vectorizer, word_embedding_model)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix = np.array([[1348,18,134,70,1,1,7,46,43],\n",
    "          [28,458,54,47,0,1,8,24,82],\n",
    "          [184,43,1038,175,1,1,26,57,136],\n",
    "          [68,20,45,1258,0,0,1,141,84],\n",
    "          [5,0,0,0,155,4,40,30,23],\n",
    "          [1,8,0,3,3,128,12,23,38],\n",
    "          [21,8,5,11,45,8,521,111,105],\n",
    "          [4,1,0,13,1,0,15,1093,29],\n",
    "          [29,41,70,101,3,37,36,105,38132]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=np.array([['B-LOC',  'B-MISC',  'B-ORG',  'B-PER',  'I-LOC',  'I-MISC',  'I-ORG',  'I-PER',  'O']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9456777953970771"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diagonal(Matrix).sum()/Matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def golden(label,matrix):\n",
    "    k=[]\n",
    "    for i in range(len(matrix)):\n",
    "        k=k+[label[0][i]]*(matrix[:,i].sum())\n",
    "    return k\n",
    "\n",
    "def predict(label,matrix):\n",
    "    k=[]\n",
    "    for i in range(len(matrix)):\n",
    "        for z in range(len(matrix)):\n",
    "            k=k+[label[0][z]]*(matrix[:,i][z])\n",
    "    return k\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=golden(label,Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_matrix=predict(label,Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9461273500257311"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_true=golden_matrix,\n",
    "                        y_pred=prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775359110733929"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_true=golden_matrix,\n",
    "                        y_pred=prediction,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9456777953970771"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_true=golden_matrix,\n",
    "                        y_pred=prediction,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9456777953970771"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_true=golden_matrix,\n",
    "                        y_pred=prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7352997074459117"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_true=golden_matrix,\n",
    "                        y_pred=prediction,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9456777953970771"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_true=golden_matrix,\n",
    "                        y_pred=prediction,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9456777953970771"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_true=golden_matrix,\n",
    "                        y_pred=prediction,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9447450521163481"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_true=golden_matrix,\n",
    "                        y_pred=prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_o = Matrix[:,-1][:-1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43,  82, 136,  84,  23,  38, 105,  29])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matrix[:,-1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_o = Matrix[-1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7572"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(golden_matrix)-Matrix[:,-1].sum()-Matrix[-1][:-1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk=np.array([[38132,Matrix[-1][:-1].sum()],\n",
    "          [Matrix[:,-1][:-1].sum(),7572]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9793854197917113"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(38132+7572)/kkk.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_list=[]\n",
    "for i in golden_matrix:\n",
    "    if i == 'O':\n",
    "        golden_list.append(i)\n",
    "    else:\n",
    "        golden_list.append('Other')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_list=[]\n",
    "for i in prediction:\n",
    "    if i == 'O':\n",
    "        pre_list.append(i)\n",
    "    else:\n",
    "        pre_list.append('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted      O  Other\n",
      "Gold                   \n",
      "O          38132    422\n",
      "Other        540   7572\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(pre_list, golden_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9793854197917113"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_true=golden_list,\n",
    "                        y_pred=pre_list,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9639068810083139"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_true=golden_list,\n",
    "                        y_pred=pre_list,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = pd.read_csv('/Users/hernando/Desktop/NLP/assignment3/interpret_lstm_output/lstm_output_10percent.tsv',sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Golden</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOCCER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JAPAN</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GET</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LUCKY</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word Golden Prediction\n",
       "0  SOCCER      O          O\n",
       "1       -      O          O\n",
       "2   JAPAN  B-LOC          O\n",
       "3     GET      O          O\n",
       "4   LUCKY      O          O"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error.columns=['Word','Golden','Prediction']\n",
    "error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = error[(error['Golden']=='B-PER')|(error['Golden']=='I-PER')]\n",
    "ori_length_name=[]\n",
    "for i in name_list.Word.values:\n",
    "    ori_length_name.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5: 193,\n",
       "         4: 79,\n",
       "         8: 61,\n",
       "         9: 29,\n",
       "         6: 146,\n",
       "         10: 18,\n",
       "         3: 43,\n",
       "         7: 89,\n",
       "         11: 10,\n",
       "         13: 1,\n",
       "         14: 1,\n",
       "         2: 39,\n",
       "         15: 1})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.Counter(ori_length_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = error[error['Golden']!=error['Prediction']].dropna().reset_index(drop=True)\n",
    "difference = difference.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "namewrong = difference[(difference['Golden']=='B-PER')|(difference['Golden']=='I-PER')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_name=[]\n",
    "for i in namewrong.Word.values:\n",
    "    length_name.append(len(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5: 56,\n",
       "         4: 27,\n",
       "         8: 26,\n",
       "         9: 11,\n",
       "         6: 42,\n",
       "         10: 13,\n",
       "         3: 3,\n",
       "         7: 35,\n",
       "         11: 7,\n",
       "         14: 1,\n",
       "         2: 1})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.Counter(length_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('B-LOC', 'O'): 15,\n",
       "         ('B-PER', 'O'): 67,\n",
       "         ('I-PER', 'B-PER'): 6,\n",
       "         ('B-LOC', 'B-ORG'): 20,\n",
       "         ('B-LOC', 'B-PER'): 3,\n",
       "         ('B-MISC', 'O'): 8,\n",
       "         ('B-PER', 'B-ORG'): 33,\n",
       "         ('I-PER', 'I-ORG'): 9,\n",
       "         ('O', 'B-MISC'): 12,\n",
       "         ('I-PER', 'O'): 82,\n",
       "         ('B-MISC', 'B-LOC'): 5,\n",
       "         ('B-MISC', 'B-ORG'): 5,\n",
       "         ('I-PER', 'B-MISC'): 1,\n",
       "         ('O', 'B-ORG'): 18,\n",
       "         ('O', 'B-LOC'): 15,\n",
       "         ('B-ORG', 'B-MISC'): 2,\n",
       "         ('B-PER', 'B-LOC'): 12,\n",
       "         ('B-PER', 'B-MISC'): 10,\n",
       "         ('I-MISC', 'B-MISC'): 3,\n",
       "         ('O', 'I-ORG'): 15,\n",
       "         ('O', 'I-PER'): 10,\n",
       "         ('O', 'B-PER'): 8,\n",
       "         ('I-MISC', 'O'): 4,\n",
       "         ('I-MISC', 'B-ORG'): 1,\n",
       "         ('B-ORG', 'O'): 17,\n",
       "         ('O', 'I-MISC'): 2,\n",
       "         ('B-ORG', 'B-LOC'): 5,\n",
       "         ('I-ORG', 'O'): 4,\n",
       "         ('B-ORG', 'B-PER'): 5,\n",
       "         ('I-ORG', 'B-ORG'): 5,\n",
       "         ('I-ORG', 'I-LOC'): 1,\n",
       "         ('B-LOC', 'B-MISC'): 4,\n",
       "         ('B-MISC', 'B-PER'): 1,\n",
       "         ('I-PER', 'I-MISC'): 1,\n",
       "         ('I-LOC', 'O'): 8,\n",
       "         ('I-MISC', 'I-LOC'): 1,\n",
       "         ('O', 'I-LOC'): 2,\n",
       "         ('B-LOC', 'I-LOC'): 1,\n",
       "         ('I-PER', 'B-ORG'): 1,\n",
       "         ('I-LOC', 'B-MISC'): 1,\n",
       "         ('I-MISC', 'B-PER'): 1,\n",
       "         ('I-MISC', 'I-ORG'): 1,\n",
       "         ('I-LOC', 'I-ORG'): 2,\n",
       "         ('B-MISC', 'I-MISC'): 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkk=[(difference['Golden'][i],difference['Prediction'][i]) for i in range(len(difference['Prediction']))]\n",
    "cs.Counter(kkk)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_first_assumption(error):\n",
    "    \n",
    "\n",
    "\n",
    "    number_of_word_list=[]\n",
    "    number_of_right_list=[]\n",
    "    number_of_word = 0\n",
    "    number_of_right = 0\n",
    "    for i in range(len(error)):\n",
    "        if error['Word'][i] == '----------':\n",
    "            number_of_word_list.append(number_of_word)\n",
    "            number_of_right_list.append(number_of_right)\n",
    "            number_of_word = 0\n",
    "            number_of_right = 0\n",
    "        else:\n",
    "            number_of_word +=1\n",
    "            if error['Golden'][i]==error['Prediction'][i]:\n",
    "                number_of_right+=1\n",
    "    sentence=pd.DataFrame(columns=['Length_of_sentence','Correct_prediction','Accuracy'])\n",
    "    sentence['Length_of_sentence'] = number_of_word_list\n",
    "    sentence['Correct_prediction'] = number_of_right_list\n",
    "    sentence['Accuracy'] = sentence['Correct_prediction']/sentence['Length_of_sentence']  \n",
    "    return sentence\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=check_first_assumption(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 350 artists>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(sentence.Length_of_sentence,sentence.Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER     O\n",
      "Gold                                                                     \n",
      "B-LOC        192       4     20      3      1       0      0      0    15\n",
      "B-MISC         5      38      5      1      0       1      0      0     8\n",
      "B-ORG          5       2     36      5      0       0      0      0    17\n",
      "B-PER         12      10     33    279      0       0      0      0    67\n",
      "I-LOC          0       1      0      0     33       0      2      0     8\n",
      "I-MISC         0       3      1      1      1      26      1      0     4\n",
      "I-ORG          0       0      5      0      1       0     15      0     4\n",
      "I-PER          0       1      1      6      0       1      9    209    82\n",
      "O             15      12     18      8      2       2     15     10  3313\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(error.Prediction.values, error.Golden.values)\n",
    "# print_precision_recall_fscore(error.Prediction.values, error.Golden.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Golden</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOCCER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JAPAN</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GET</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LUCKY</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50239</th>\n",
       "      <td>\"</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50240</th>\n",
       "      <td>he</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50241</th>\n",
       "      <td>added</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50242</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50243</th>\n",
       "      <td>----------</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50244 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word Golden Prediction\n",
       "0          SOCCER      O          O\n",
       "1               -      O          O\n",
       "2           JAPAN  B-LOC          O\n",
       "3             GET      O          O\n",
       "4           LUCKY      O          O\n",
       "...           ...    ...        ...\n",
       "50239           \"      O          O\n",
       "50240          he      O          O\n",
       "50241       added      O          O\n",
       "50242           .      O          O\n",
       "50243  ----------    NaN        NaN\n",
       "\n",
       "[50244 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final= pd.read_csv('/Users/hernando/Desktop/NLP/assignment3/interpret_lstm_output/lstm_output_all.tsv', delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8',header=None)\n",
    "final.columns=['Word','Golden','Prediction']\n",
    "\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_error=final[final['Golden']!=final['Prediction']].dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz=[(final_error['Golden'][i],final_error['Prediction'][i]) for i in range(len(final_error['Prediction']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('B-LOC', 'O'): 155,\n",
       "         ('B-PER', 'O'): 388,\n",
       "         ('I-PER', 'B-PER'): 50,\n",
       "         ('B-LOC', 'B-ORG'): 122,\n",
       "         ('B-LOC', 'B-PER'): 36,\n",
       "         ('B-MISC', 'O'): 142,\n",
       "         ('B-PER', 'B-ORG'): 148,\n",
       "         ('I-PER', 'I-ORG'): 66,\n",
       "         ('O', 'B-MISC'): 211,\n",
       "         ('I-PER', 'O'): 330,\n",
       "         ('B-MISC', 'B-LOC'): 31,\n",
       "         ('B-MISC', 'B-ORG'): 51,\n",
       "         ('I-PER', 'B-MISC'): 6,\n",
       "         ('O', 'B-ORG'): 396,\n",
       "         ('O', 'B-LOC'): 183,\n",
       "         ('B-ORG', 'B-MISC'): 79,\n",
       "         ('B-PER', 'B-LOC'): 70,\n",
       "         ('B-PER', 'B-MISC'): 44,\n",
       "         ('I-MISC', 'B-MISC'): 14,\n",
       "         ('O', 'I-ORG'): 152,\n",
       "         ('O', 'I-PER'): 56,\n",
       "         ('O', 'B-PER'): 138,\n",
       "         ('I-MISC', 'O'): 44,\n",
       "         ('I-MISC', 'B-ORG'): 6,\n",
       "         ('B-ORG', 'O'): 344,\n",
       "         ('O', 'I-MISC'): 82,\n",
       "         ('B-ORG', 'B-LOC'): 142,\n",
       "         ('I-ORG', 'O'): 120,\n",
       "         ('B-ORG', 'B-PER'): 54,\n",
       "         ('I-ORG', 'B-ORG'): 36,\n",
       "         ('I-ORG', 'I-LOC'): 32,\n",
       "         ('B-LOC', 'B-MISC'): 62,\n",
       "         ('B-MISC', 'B-PER'): 12,\n",
       "         ('I-PER', 'I-MISC'): 7,\n",
       "         ('I-LOC', 'O'): 39,\n",
       "         ('I-MISC', 'I-LOC'): 3,\n",
       "         ('O', 'I-LOC'): 25,\n",
       "         ('B-LOC', 'I-LOC'): 3,\n",
       "         ('I-PER', 'B-ORG'): 16,\n",
       "         ('I-LOC', 'B-MISC'): 1,\n",
       "         ('I-MISC', 'B-PER'): 3,\n",
       "         ('I-MISC', 'I-ORG'): 13,\n",
       "         ('I-LOC', 'I-ORG'): 50,\n",
       "         ('B-MISC', 'I-MISC'): 3,\n",
       "         ('I-PER', 'B-LOC'): 5,\n",
       "         ('I-LOC', 'I-MISC'): 3,\n",
       "         ('I-ORG', 'B-PER'): 8,\n",
       "         ('I-ORG', 'I-PER'): 17,\n",
       "         ('B-ORG', 'I-ORG'): 27,\n",
       "         ('B-PER', 'I-PER'): 20,\n",
       "         ('B-PER', 'I-ORG'): 16,\n",
       "         ('I-ORG', 'I-MISC'): 25,\n",
       "         ('I-ORG', 'B-LOC'): 13,\n",
       "         ('I-LOC', 'B-LOC'): 6,\n",
       "         ('I-ORG', 'B-MISC'): 12,\n",
       "         ('B-PER', 'I-MISC'): 5,\n",
       "         ('I-MISC', 'I-PER'): 5,\n",
       "         ('B-ORG', 'I-PER'): 1,\n",
       "         ('I-PER', 'I-LOC'): 10,\n",
       "         ('B-MISC', 'I-LOC'): 3,\n",
       "         ('I-LOC', 'I-PER'): 6,\n",
       "         ('I-LOC', 'B-PER'): 3,\n",
       "         ('B-LOC', 'I-ORG'): 5,\n",
       "         ('B-MISC', 'I-ORG'): 2,\n",
       "         ('B-MISC', 'I-PER'): 1,\n",
       "         ('B-ORG', 'I-MISC'): 4,\n",
       "         ('B-LOC', 'I-MISC'): 1,\n",
       "         ('B-ORG', 'I-LOC'): 1,\n",
       "         ('B-PER', 'I-LOC'): 3})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.Counter(zzz) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalsentence = check_first_assumption(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result=finalsentence.groupby(['Length_of_sentence']).agg({'Accuracy':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x936 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_result.plot(kind='bar',figsize=(15,13))\n",
    "plt.xlabel('Length_of_sentence',size=25)\n",
    "plt.ylabel('Accuracy',size=25)\n",
    "plt.legend(fontsize=15)\n",
    "plt.savefig('accuracy_length.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_second_assumption(error):\n",
    "    \n",
    "    name_error =  error[error['Golden']!=error['Prediction']].dropna().reset_index(drop=True)\n",
    "    print('Total wrong preidction',len(name_error['Golden']))\n",
    "    \n",
    "    wrong_name_prediction = name_error[(name_error['Golden']=='B-PER')|(name_error['Golden']=='I-PER')]\n",
    "    \n",
    "    print('Total wrong name preidction',len(wrong_name_prediction['Golden']))\n",
    "    print('Wrong name prediction percentage',len(wrong_name_prediction['Golden'])*100/len(name_error['Golden']))\n",
    "    wrong = 0\n",
    "    for i in wrong_name_prediction.Word.values:\n",
    "        if len(i)>5:\n",
    "            wrong+=1\n",
    "            \n",
    "    print('Wrong 5 name prediction',wrong, wrong*100/len(wrong_name_prediction['Golden']))\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "#     number_of_word = 0\n",
    "#     number_of_wrong = 0\n",
    "#     number_of_right = 0\n",
    "#     for i in range(len(error)):\n",
    "#         if error['Word'][i] != '----------':\n",
    "# #             number_of_word_list.append(number_of_word)\n",
    "# #             number_of_right_list.append(number_of_right)\n",
    "#             number_of_word += 1\n",
    "# #         else:\n",
    "# #             number_of_word +=1\n",
    "#         if error['Golden'][i] == 'B-PER' and error['Prediction'][i] != 'B-PER':\n",
    "#             number_of_wrong+=1\n",
    "#         if error['Golden'][i] == 'I-PER' and error['Prediction'][i] != 'I-PER':\n",
    "#             number_of_wrong+=1\n",
    "#         if error['Golden'][i] ==  error['Prediction'][i]:\n",
    "#             number_of_right+=1\n",
    "#     sentence=pd.DataFrame(columns=['Length_of_sentence','Correct_prediction','Accuracy'])\n",
    "#     sentence['Length_of_sentence'] = number_of_word_list\n",
    "#     sentence['Correct_prediction'] = number_of_right_list\n",
    "#     sentence['Accuracy'] = sentence['Correct_prediction']/sentence['Length_of_sentence']  \n",
    "    return \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wrong preidction 4167\n",
      "Total wrong name preidction 1184\n",
      "Wrong name prediction percentage 28.413726901847852\n",
      "Wrong 5 name prediction 693 58.5304054054054\n"
     ]
    }
   ],
   "source": [
    "check_second_assumption(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_third_assumption(error):\n",
    "#     poss = nltk.pos_tag(error.Word.values)\n",
    "#     poss_list=[]\n",
    "#     for i in poss:\n",
    "#         poss_list.append(i[1])\n",
    "#     error['pos']=poss_list\n",
    "#     total=0\n",
    "#     true=0\n",
    "#     for i in range(len(error)):\n",
    "#         if error['pos'][i] =='JJ' and error['pos'][i+1] =='NNP':\n",
    "#             total+=1\n",
    "#             if error['Golden'][i+1] != error['Prediction'][i+1]:\n",
    "#                 true +=1\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "#     return total, true\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1138, 573)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check_third_assumption(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adversarial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_name =final[(final['Golden']=='B-PER')|(final['Golden']=='I-PER')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name=[]\n",
    "for i in final_name.Word.values:\n",
    "    if len(i)<=5:\n",
    "        list_name.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frode'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choices(list_name)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_example = final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(adversarial_example)):\n",
    "    if adversarial_example['Golden'][i]=='B-PER' or adversarial_example['Golden'][i]=='I-PER':\n",
    "        if len(adversarial_example['Word'][i])>5:\n",
    "            adversarial_example['Word'][i] = random.choices(list_name)[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Golden</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHINA</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nadim</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ladki</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Igor</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Dion</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50114</th>\n",
       "      <td>Peggy</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50130</th>\n",
       "      <td>Dick</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50131</th>\n",
       "      <td>Dejan</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50142</th>\n",
       "      <td>de</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50206</th>\n",
       "      <td>Aung</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2771 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word Golden Prediction\n",
       "7      CHINA  B-PER      B-PER\n",
       "13     Nadim  B-PER          O\n",
       "14     Ladki  I-PER      B-PER\n",
       "93      Igor  B-PER      B-ORG\n",
       "94      Dion  I-PER      I-ORG\n",
       "...      ...    ...        ...\n",
       "50114  Peggy  B-PER          O\n",
       "50130   Dick  B-PER      B-PER\n",
       "50131  Dejan  I-PER      I-PER\n",
       "50142     de  B-PER      B-LOC\n",
       "50206   Aung  B-PER      B-ORG\n",
       "\n",
       "[2771 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversarial_example[(adversarial_example['Golden']=='B-PER')|(adversarial_example['Golden']=='I-PER')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Golden</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHINA</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nadim</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ladki</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Igor</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Shkvyrin</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50114</th>\n",
       "      <td>Peggy</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50130</th>\n",
       "      <td>Dick</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50131</th>\n",
       "      <td>Spring</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50142</th>\n",
       "      <td>Charlton</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50206</th>\n",
       "      <td>Charlton</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2771 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word Golden Prediction\n",
       "7         CHINA  B-PER      B-PER\n",
       "13        Nadim  B-PER          O\n",
       "14        Ladki  I-PER      B-PER\n",
       "93         Igor  B-PER      B-ORG\n",
       "94     Shkvyrin  I-PER      I-ORG\n",
       "...         ...    ...        ...\n",
       "50114     Peggy  B-PER          O\n",
       "50130      Dick  B-PER      B-PER\n",
       "50131    Spring  I-PER      I-PER\n",
       "50142  Charlton  B-PER      B-LOC\n",
       "50206  Charlton  B-PER      B-ORG\n",
       "\n",
       "[2771 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[(final['Golden']=='B-PER')|(final['Golden']=='I-PER')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_example[[\"Word\"]].to_csv(\"adversarial_example.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
